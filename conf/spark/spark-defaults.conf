# Spark 默认配置文件
# 适用于与 Hadoop 伪分布式环境集成

# Spark 应用配置
spark.app.name=SparkOnHadoop
spark.master=spark://spark-master:7077
spark.submit.deployMode=client

# HDFS 集成配置
spark.hadoop.fs.defaultFS=hdfs://namenode:9000
spark.hadoop.dfs.replication=1
spark.hadoop.dfs.client.use.datanode.hostname=true

# 内存配置 - 基于容器资源限制优化
spark.driver.memory=512m
spark.executor.memory=768m
spark.executor.cores=1
spark.executor.instances=1

# 序列化配置
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# 性能优化
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.cbo.enabled=true

# 并行度配置 - 适配单节点环境
spark.default.parallelism=2
spark.sql.shuffle.partitions=2

# 动态资源分配 - 在资源受限环境中启用
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=2
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.cachedExecutorIdleTimeout=120s

# 内存管理
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5

# 日志配置
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://namenode:9000/spark-logs
spark.history.fs.logDirectory=hdfs://namenode:9000/spark-logs

# 临时目录配置
spark.local.dir=/tmp/spark-local
spark.worker.dir=/opt/spark/work

# 网络配置 - 适配 Docker 网络环境
spark.driver.host=spark-master
spark.driver.bindAddress=0.0.0.0
spark.blockManager.port=10000
spark.driver.blockManager.port=10001

# 调度配置
spark.scheduler.mode=FAIR
spark.locality.wait=3s

# 垃圾回收优化
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap
spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap

# 安全配置 - 在测试环境中简化
spark.authenticate=false
spark.network.crypto.enabled=false
spark.ssl.enabled=false
spark.ui.enabled=true
spark.ui.port=4040

# 兼容性配置
spark.sql.legacy.timeParserPolicy=LEGACY
spark.sql.legacy.allowUntypedScalaUDF=true