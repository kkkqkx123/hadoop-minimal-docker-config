version: '3.8'

# Spark Standalone 模式集成配置
# 与现有 Hadoop 伪分布式环境配合使用

services:
  # Spark Master 节点
  spark-master:
    image: apache/spark:v3.5.3
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master RPC
      - "6066:6066"  # Spark Master REST API
    volumes:
      - ./conf/spark:/opt/spark/conf
      - ./data/spark-logs:/opt/spark/logs
      - ./data/spark-work:/opt/spark/work
      - ./conf:/opt/hadoop/etc/hadoop:ro
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.2'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      - namenode
      - datanode
    restart: unless-stopped

  # Spark Worker 节点
  spark-worker:
    image: apache/spark:v3.5.3
    container_name: spark-worker
    hostname: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1g
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_PORT=8888
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8081"  # Spark Worker Web UI
      - "8888:8888"  # Spark Worker RPC
    volumes:
      - ./conf/spark:/opt/spark/conf
      - ./data/spark-logs:/opt/spark/logs
      - ./data/spark-work:/opt/spark/work
      - ./conf:/opt/hadoop/etc/hadoop:ro
    networks:
      - hadoop-network
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      spark-master:
        condition: service_healthy
    restart: unless-stopped

networks:
  hadoop-network:
    external: true

# 数据卷配置
volumes:
  spark-logs:
    driver: local
  spark-work:
    driver: local
  spark-history:
    driver: local