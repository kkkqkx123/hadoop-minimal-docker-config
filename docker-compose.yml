version: '3.8'

services:
  hadoop-pseudo:
    image: hadoop:optimized
    container_name: hadoop-pseudo
    hostname: hadoop-pseudo
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8088:8088"
      - "19888:19888"
    volumes:
      - ./conf:/opt/hadoop/etc/hadoop
      - ./data:/hadoop/dfs
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - JAVA_HOME=/usr/lib/jvm/jre
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: '1.0'
    command: ["/bin/bash", "-c", "hdfs namenode -format -force && hdfs --daemon start namenode && hdfs --daemon start datanode && yarn --daemon start resourcemanager && yarn --daemon start nodemanager && tail -f /dev/null"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # 单节点Spark（可选，独立运行）
  spark-master:
    image: apache/spark:3.5.7
    container_name: spark-master
    hostname: spark-master
    deploy:
      resources:
        limits:
          memory: 512M    # 内存限制
          cpus: '0.5'     # CPU限制
        reservations:
          memory: 256M    # 内存预留
          cpus: '0.2'     # CPU预留
    environment:
      # 设置Hadoop环境变量
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - HADOOP_HOME=/opt/hadoop
      # 设置Spark类路径，避免执行Hadoop命令
      - SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop
      # Spark Master配置
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      # 安全配置
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "-h", "spark-master", "-p", "7077", "--webui-port", "8080"]
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master RPC端口
      - "6066:6066"   # Spark Master REST端口
    volumes:
      - ./conf/spark:/opt/spark/conf
      - ./data:/data
      - ./conf:/opt/hadoop/etc/hadoop:ro
    networks:
      - hadoop
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      hadoop-pseudo:
        condition: service_healthy
    restart: unless-stopped

volumes:
  namenode:
  datanode:

networks:
  hadoop:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1450