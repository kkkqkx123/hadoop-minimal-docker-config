# æœ¬åœ°Mockæµ‹è¯•æŒ‡å—

## ğŸ¯ æ¦‚è¿°

åœ¨Dockerç¯å¢ƒä¸­è°ƒè¯•MapReduceç¨‹åºå¯èƒ½ä¼šé‡åˆ°ç¯å¢ƒå¤æ‚ã€è°ƒè¯•å›°éš¾çš„é—®é¢˜ã€‚æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨æœ¬åœ°ç¯å¢ƒä¸­é€šè¿‡Mockæµ‹è¯•æ¥éªŒè¯ä»£ç é€»è¾‘ï¼Œé¿å…åœ¨Dockerä¸Šå‘ç°é—®é¢˜åéš¾ä»¥è°ƒè¯•çš„æƒ…å†µã€‚

## ğŸ§ª Mockæµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•ï¼ˆUnit Testingï¼‰

#### Python MapReduce Mockæµ‹è¯•

**mapper_mock_test.py**
```python
#!/usr/bin/env python3
"""Mapperå•å…ƒæµ‹è¯•"""

import sys
import io
from unittest.mock import patch

def test_mapper():
    """æµ‹è¯•mapperå‡½æ•°"""
    # Mockè¾“å…¥æ•°æ®
    test_input = "hello world hello hadoop\nthis is a test"
    
    # æ•è·è¾“å‡º
    output = io.StringIO()
    
    # é‡å®šå‘stdinå’Œstdout
    with patch('sys.stdin', io.StringIO(test_input)):
        with patch('sys.stdout', output):
            # å¯¼å…¥å¹¶æ‰§è¡Œmapper
            exec(open('mapper.py').read())
    
    # éªŒè¯è¾“å‡º
    result = output.getvalue().strip().split('\n')
    expected_words = ['hello', 'world', 'hello', 'hadoop', 'this', 'is', 'test']
    
    print("Mapperæµ‹è¯•ç»“æœ:")
    for line in result:
        word, count = line.split('\t')
        print(f"  {word}: {count}")
        assert word in expected_words, f"æ„å¤–å•è¯: {word}"
        assert count == '1', f"è®¡æ•°é”™è¯¯: {count}"
    
    print("âœ… Mapperæµ‹è¯•é€šè¿‡")

def test_reducer():
    """æµ‹è¯•reducerå‡½æ•°"""
    # Mockè¾“å…¥æ•°æ®ï¼ˆå·²æ’åºï¼‰
    test_input = "hello\t1\nhello\t1\nhadoop\t1\nworld\t1"
    
    # æ•è·è¾“å‡º
    output = io.StringIO()
    
    # é‡å®šå‘stdinå’Œstdout
    with patch('sys.stdin', io.StringIO(test_input)):
        with patch('sys.stdout', output):
            # å¯¼å…¥å¹¶æ‰§è¡Œreducer
            exec(open('reducer.py').read())
    
    # éªŒè¯è¾“å‡º
    result = output.getvalue().strip().split('\n')
    expected = {'hello': '2', 'hadoop': '1', 'world': '1'}
    
    print("\nReduceræµ‹è¯•ç»“æœ:")
    for line in result:
        word, count = line.split('\t')
        print(f"  {word}: {count}")
        assert word in expected, f"æ„å¤–å•è¯: {word}"
        assert count == expected[word], f"è®¡æ•°é”™è¯¯: {count}"
    
    print("âœ… Reduceræµ‹è¯•é€šè¿‡")

if __name__ == '__main__':
    test_mapper()
    test_reducer()
    print("\nğŸ‰ æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼")
```

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
# è¿è¡Œå•å…ƒæµ‹è¯•
python3 mapper_mock_test.py

# é›†æˆåˆ°æµ‹è¯•æ¡†æ¶
python3 -m pytest mapper_mock_test.py -v
```

#### Java MapReduce Mockæµ‹è¯•

**WordCountMapperTest.java**
```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.junit.Test;
import org.junit.Before;
import static org.mockito.Mockito.*;

public class WordCountMapperTest {
    
    private WordCountMapper mapper;
    private Mapper.Context mockContext;
    
    @Before
    public void setUp() {
        mapper = new WordCountMapper();
        mockContext = mock(Mapper.Context.class);
    }
    
    @Test
    public void testMap() throws Exception {
        // æµ‹è¯•è¾“å…¥
        Text inputKey = new Text("1");  // è¡Œå·
        Text inputValue = new Text("hello world hello hadoop");
        
        // æ‰§è¡Œmapå‡½æ•°
        mapper.map(inputKey, inputValue, mockContext);
        
        // éªŒè¯è¾“å‡º
        verify(mockContext, times(2)).write(new Text("hello"), new IntWritable(1));
        verify(mockContext, times(1)).write(new Text("world"), new IntWritable(1));
        verify(mockContext, times(1)).write(new Text("hadoop"), new IntWritable(1));
    }
    
    @Test
    public void testMapWithEmptyInput() throws Exception {
        Text inputKey = new Text("1");
        Text inputValue = new Text("");
        
        mapper.map(inputKey, inputValue, mockContext);
        
        // éªŒè¯æ²¡æœ‰è¾“å‡º
        verify(mockContext, never()).write(any(), any());
    }
}
```

### 2. é›†æˆæµ‹è¯•ï¼ˆIntegration Testingï¼‰

#### æœ¬åœ°Pipelineæµ‹è¯•

**local_pipeline_test.py**
```python
#!/usr/bin/env python3
"""æœ¬åœ°Pipelineé›†æˆæµ‹è¯•"""

import subprocess
import tempfile
import os

def run_local_pipeline(input_data, mapper_script, reducer_script):
    """åœ¨æœ¬åœ°è¿è¡Œå®Œæ•´çš„MapReduceæµç¨‹"""
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write(input_data)
        input_file = f.name
    
    try:
        # æ­¥éª¤1: è¿è¡Œmapper
        with open(input_file, 'r') as infile:
            mapper_result = subprocess.run(
                ['python3', mapper_script],
                stdin=infile,
                capture_output=True,
                text=True
            )
        
        if mapper_result.returncode != 0:
            print(f"Mapperæ‰§è¡Œå¤±è´¥: {mapper_result.stderr}")
            return None
        
        # æ­¥éª¤2: æ’åºï¼ˆæ¨¡æ‹Ÿshuffleé˜¶æ®µï¼‰
        sorted_output = '\n'.join(sorted(mapper_result.stdout.strip().split('\n')))
        
        # æ­¥éª¤3: è¿è¡Œreducer
        reducer_result = subprocess.run(
            ['python3', reducer_script],
            input=sorted_output,
            capture_output=True,
            text=True
        )
        
        if reducer_result.returncode != 0:
            print(f"Reduceræ‰§è¡Œå¤±è´¥: {reducer_result.stderr}")
            return None
        
        return reducer_result.stdout
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(input_file)

def test_wordcount_pipeline():
    """æµ‹è¯•è¯é¢‘ç»Ÿè®¡pipeline"""
    
    # æµ‹è¯•æ•°æ®
    test_data = """hello world hello hadoop
this is a test file for word count
hadoop is great for big data processing
hello hadoop users"""
    
    # è¿è¡Œæœ¬åœ°pipeline
    result = run_local_pipeline(test_data, 'mapper.py', 'reducer.py')
    
    if result is None:
        print("âŒ Pipelineæµ‹è¯•å¤±è´¥")
        return False
    
    # è§£æç»“æœ
    print("Pipelineæµ‹è¯•ç»“æœ:")
    word_counts = {}
    for line in result.strip().split('\n'):
        if line:
            word, count = line.split('\t')
            word_counts[word] = int(count)
            print(f"  {word}: {count}")
    
    # éªŒè¯ç»“æœ
    expected = {
        'hello': 3,
        'hadoop': 3,
        'world': 1,
        'this': 1,
        'is': 2,
        'test': 1,
        'file': 1,
        'for': 1,
        'word': 1,
        'count': 1,
        'great': 1,
        'big': 1,
        'data': 1,
        'processing': 1,
        'users': 1
    }
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…
    if word_counts == expected:
        print("âœ… Pipelineæµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âŒ Pipelineç»“æœä¸åŒ¹é…")
        print("æœŸæœ›:", expected)
        print("å®é™…:", word_counts)
        return False

def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    
    test_cases = [
        ("", "ç©ºè¾“å…¥æµ‹è¯•"),
        ("   \n  \n", "ç©ºç™½è¡Œæµ‹è¯•"),
        ("HELLO Hello hello", "å¤§å°å†™æµ‹è¯•"),
        ("test123 test!@# test...", "ç‰¹æ®Šå­—ç¬¦æµ‹è¯•"),
        ("a\nb\nc", "å•å­—ç¬¦æµ‹è¯•")
    ]
    
    print("\nè¾¹ç•Œæƒ…å†µæµ‹è¯•:")
    all_passed = True
    
    for test_data, description in test_cases:
        print(f"\n{description}:")
        result = run_local_pipeline(test_data, 'mapper.py', 'reducer.py')
        
        if result is not None:
            print(f"  ç»“æœ: {result.strip()}")
            print("  âœ… é€šè¿‡")
        else:
            print("  âŒ å¤±è´¥")
            all_passed = False
    
    return all_passed

if __name__ == '__main__':
    print("ğŸ§ª æœ¬åœ°Pipelineé›†æˆæµ‹è¯•")
    print("=" * 40)
    
    # è¿è¡Œæµ‹è¯•
    pipeline_passed = test_wordcount_pipeline()
    edge_passed = test_edge_cases()
    
    if pipeline_passed and edge_passed:
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘")
```

### 3. æ•°æ®éªŒè¯æµ‹è¯•

**data_validation_test.py**
```python
#!/usr/bin/env python3
"""æ•°æ®éªŒè¯æµ‹è¯•"""

import json
import hashlib
from typing import Dict, List, Tuple

def validate_mapper_output(output_lines: List[str]) -> bool:
    """éªŒè¯mapperè¾“å‡ºæ ¼å¼"""
    
    print("éªŒè¯mapperè¾“å‡ºæ ¼å¼...")
    
    for line in output_lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ ¼å¼ï¼šword\tcount
        parts = line.split('\t')
        if len(parts) != 2:
            print(f"âŒ æ ¼å¼é”™è¯¯: {line}")
            return False
        
        word, count = parts
        
        # éªŒè¯countæ˜¯æ•°å­—
        try:
            int(count)
        except ValueError:
            print(f"âŒ è®¡æ•°ä¸æ˜¯æ•°å­—: {count}")
            return False
    
    print("âœ… Mapperè¾“å‡ºæ ¼å¼æ­£ç¡®")
    return True

def validate_reducer_input(output_lines: List[str]) -> bool:
    """éªŒè¯reducerè¾“å…¥æ ¼å¼ï¼ˆå·²æ’åºï¼‰"""
    
    print("éªŒè¯reducerè¾“å…¥æ ¼å¼...")
    
    current_word = None
    current_count = 0
    
    for line in output_lines:
        line = line.strip()
        if not line:
            continue
            
        parts = line.split('\t')
        if len(parts) != 2:
            print(f"âŒ æ ¼å¼é”™è¯¯: {line}")
            return False
        
        word, count = parts
        count = int(count)
        
        # æ£€æŸ¥æ˜¯å¦æŒ‰å•è¯åˆ†ç»„
        if current_word is None:
            current_word = word
            current_count = count
        elif word == current_word:
            current_count += count
        else:
            # æ–°å•è¯ï¼ŒéªŒè¯å‰ä¸€ä¸ªå•è¯çš„å®Œæ•´æ€§
            print(f"  å•è¯ '{current_word}' æ€»è®¡æ•°: {current_count}")
            current_word = word
            current_count = count
    
    if current_word:
        print(f"  å•è¯ '{current_word}' æ€»è®¡æ•°: {current_count}")
    
    print("âœ… Reducerè¾“å…¥æ ¼å¼æ­£ç¡®")
    return True

def generate_test_report(mapper_input: str, mapper_output: str, 
                        reducer_input: str, reducer_output: str) -> Dict:
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    
    report = {
        "input_stats": {
            "lines": len(mapper_input.strip().split('\n')),
            "characters": len(mapper_input),
            "words": len(mapper_input.split())
        },
        "mapper_stats": {
            "output_lines": len([l for l in mapper_output.split('\n') if l.strip()]),
            "unique_words": len(set(line.split('\t')[0] for line in mapper_output.split('\n') if line.strip() and '\t' in line))
        },
        "reducer_stats": {
            "output_lines": len([l for l in reducer_output.split('\n') if l.strip()]),
            "total_words": sum(int(line.split('\t')[1]) for line in reducer_output.split('\n') if line.strip() and '\t' in line)
        }
    }
    
    # è®¡ç®—æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    mapper_total = sum(int(line.split('\t')[1]) for line in mapper_output.split('\n') 
                        if line.strip() and '\t' in line)
    reducer_total = sum(int(line.split('\t')[1]) for line in reducer_output.split('\n') 
                        if line.strip() and '\t' in line)
    
    report["data_integrity"] = {
        "mapper_total_count": mapper_total,
        "reducer_total_count": reducer_total,
        "counts_match": mapper_total == reducer_total
    }
    
    return report

def save_test_report(report: Dict, filename: str = "test_report.json"):
    """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

if __name__ == '__main__':
    print("ğŸ“Š æ•°æ®éªŒè¯æµ‹è¯•")
    print("=" * 30)
    
    # ç¤ºä¾‹æ•°æ®
    sample_mapper_output = """hello\t1
world\t1
hello\t1
hadoop\t1"""
    
    sample_reducer_input = """hello\t1
hello\t1
hadoop\t1
world\t1"""
    
    # è¿è¡ŒéªŒè¯
    validate_mapper_output(sample_mapper_output.split('\n'))
    validate_reducer_input(sample_reducer_input.split('\n'))
    
    print("\nğŸ‰ æ•°æ®éªŒè¯å®Œæˆ")
```

## ğŸ”§ æµ‹è¯•ç¯å¢ƒæ­å»º

### å®‰è£…ä¾èµ–
```bash
# Pythonæµ‹è¯•ä¾èµ–
pip install pytest pytest-mock

# Javaæµ‹è¯•ä¾èµ–ï¼ˆMavené¡¹ç›®ï¼‰
<dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.13.2</version>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>org.mockito</groupId>
    <artifactId>mockito-core</artifactId>
    <version>4.6.1</version>
    <scope>test</scope>
</dependency>
```

### æµ‹è¯•ç›®å½•ç»“æ„å»ºè®®
```
your-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ java/          # Javaæºä»£ç 
â”‚   â”‚   â””â”€â”€ python/        # Pythonè„šæœ¬
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ java/          # Javaå•å…ƒæµ‹è¯•
â”‚       â”œâ”€â”€ python/        # Pythonå•å…ƒæµ‹è¯•
â”‚       â””â”€â”€ integration/   # é›†æˆæµ‹è¯•
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mapper.py
â”‚   â”œâ”€â”€ reducer.py
â”‚   â””â”€â”€ run_local_test.sh  # æœ¬åœ°æµ‹è¯•è„šæœ¬
â””â”€â”€ test-reports/          # æµ‹è¯•æŠ¥å‘Šè¾“å‡ºç›®å½•
```

## ğŸš€ è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹

**run_local_test.sh**
```bash
#!/bin/bash
# æœ¬åœ°è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹æœ¬åœ°Mockæµ‹è¯•"
echo "=================="

# 1. å•å…ƒæµ‹è¯•
echo "ğŸ“‹ è¿è¡Œå•å…ƒæµ‹è¯•..."
python3 -m pytest test/ -v --tb=short

# 2. é›†æˆæµ‹è¯•
echo "ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•..."
python3 test/python/local_pipeline_test.py

# 3. æ•°æ®éªŒè¯
echo "ğŸ” è¿è¡Œæ•°æ®éªŒè¯..."
python3 test/python/data_validation_test.py

# 4. ç”Ÿæˆæœ¬åœ°pipelineæµ‹è¯•æŠ¥å‘Š
echo "ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."
python3 test/python/generate_test_report.py

echo "âœ… æœ¬åœ°æµ‹è¯•å®Œæˆï¼"
echo "ğŸ’¡ ç°åœ¨å¯ä»¥å®‰å…¨åœ°éƒ¨ç½²åˆ°Dockerç¯å¢ƒäº†"
```

## ğŸ’¡ è°ƒè¯•æŠ€å·§

### 1. é€æ­¥è°ƒè¯•
```python
# åœ¨å…³é”®ä½ç½®æ·»åŠ è°ƒè¯•è¾“å‡º
def mapper():
    for line in sys.stdin:
        print(f"DEBUG: è¾“å…¥è¡Œ: {line}", file=sys.stderr)
        # ... å¤„ç†é€»è¾‘
        print(f"DEBUG: è¾“å‡º: {word}\t{count}", file=sys.stderr)
        print(f"{word}\t{count}")
```

### 2. æ•°æ®é‡‡æ ·æµ‹è¯•
```python
def test_with_sample_data():
    """ä½¿ç”¨å°æ•°æ®é›†å¿«é€ŸéªŒè¯"""
    sample_data = "hello world hello hadoop"
    
    # æœ¬åœ°è¿è¡Œmapper
    mapper_result = subprocess.run(['python3', 'mapper.py'], 
                                  input=sample_data, 
                                  capture_output=True, text=True)
    
    print("Mapperè¾“å‡º:")
    print(mapper_result.stdout)
    
    # æœ¬åœ°è¿è¡Œreducer
    reducer_result = subprocess.run(['python3', 'reducer.py'], 
                                   input=mapper_result.stdout, 
                                   capture_output=True, text=True)
    
    print("\nReducerè¾“å‡º:")
    print(reducer_result.stdout)
```

### 3. å¯¹æ¯”æµ‹è¯•
```python
def compare_with_expected(input_data, expected_output):
    """å¯¹æ¯”å®é™…è¾“å‡ºä¸æœŸæœ›è¾“å‡º"""
    
    # è¿è¡Œæœ¬åœ°pipeline
    actual_output = run_local_pipeline(input_data, 'mapper.py', 'reducer.py')
    
    # å¯¹æ¯”ç»“æœ
    actual_lines = set(actual_output.strip().split('\n'))
    expected_lines = set(expected_output.strip().split('\n'))
    
    if actual_lines == expected_lines:
        print("âœ… è¾“å‡ºåŒ¹é…æœŸæœ›ç»“æœ")
    else:
        print("âŒ è¾“å‡ºä¸åŒ¹é…:")
        print("é¢å¤–è¾“å‡º:", actual_lines - expected_lines)
        print("ç¼ºå¤±è¾“å‡º:", expected_lines - actual_lines)
```

## ğŸ“Š æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

**generate_test_report.py**
```python
#!/usr/bin/env python3
"""ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""

import datetime
import json
import os
from local_pipeline_test import run_local_pipeline, test_wordcount_pipeline

def generate_comprehensive_report():
    """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
    
    report = {
        "timestamp": datetime.datetime.now().isoformat(),
        "environment": {
            "python_version": "3.x",
            "test_framework": "pytest",
            "os": "Linux/MacOS/Windows"
        },
        "test_results": {},
        "recommendations": []
    }
    
    # è¿è¡Œå„ç§æµ‹è¯•
    print("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
    try:
        test_wordcount_pipeline()
        report["test_results"]["basic_functionality"] = "PASSED"
    except Exception as e:
        report["test_results"]["basic_functionality"] = f"FAILED: {str(e)}"
        report["recommendations"].append("æ£€æŸ¥åŸºç¡€MapReduceé€»è¾‘")
    
    # 2. æ€§èƒ½æµ‹è¯•ï¼ˆå°æ•°æ®é›†ï¼‰
    import time
    test_data = "hello world " * 1000  # 2000ä¸ªå•è¯
    
    start_time = time.time()
    result = run_local_pipeline(test_data, 'mapper.py', 'reducer.py')
    end_time = time.time()
    
    if result:
        report["test_results"]["performance"] = {
            "status": "PASSED",
            "execution_time": f"{end_time - start_time:.3f}s",
            "data_size": len(test_data.split()),
            "throughput": f"{len(test_data.split()) / (end_time - start_time):.1f} words/s"
        }
    else:
        report["test_results"]["performance"] = "FAILED"
        report["recommendations"].append("æ£€æŸ¥æ€§èƒ½ç“¶é¢ˆ")
    
    # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # è¿è¡Œå¤§æ•°æ®é›†æµ‹è¯•
    large_data = "test " * 10000  # 10000ä¸ªå•è¯
    run_local_pipeline(large_data, 'mapper.py', 'reducer.py')
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_increase = memory_after - memory_before
    
    report["test_results"]["memory_usage"] = {
        "memory_increase_mb": f"{memory_increase:.2f}",
        "status": "PASSED" if memory_increase < 100 else "WARNING"
    }
    
    if memory_increase > 100:
        report["recommendations"].append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "test_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*50)
    print("æµ‹è¯•æŠ¥å‘Šæ‘˜è¦:")
    print("="*50)
    
    for test_name, result in report["test_results"].items():
        if isinstance(result, dict):
            status = result.get("status", "UNKNOWN")
            print(f"{test_name}: {status}")
            for key, value in result.items():
                if key != "status":
                    print(f"  {key}: {value}")
        else:
            print(f"{test_name}: {result}")
    
    if report["recommendations"]:
        print("\nå»ºè®®:")
        for rec in report["recommendations"]:
            print(f"  - {rec}")
    
    return report

if __name__ == '__main__':
    generate_comprehensive_report()
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆTDDï¼‰
```
1. å…ˆç¼–å†™æµ‹è¯•ç”¨ä¾‹
2. è¿è¡Œæµ‹è¯•ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
3. ç¼–å†™æœ€å°åŒ–çš„å®ç°ä»£ç 
4. è¿è¡Œæµ‹è¯•ï¼ˆåº”è¯¥é€šè¿‡ï¼‰
5. é‡æ„ä¼˜åŒ–ä»£ç 
6. é‡å¤ä»¥ä¸Šæ­¥éª¤
```

### 2. æµ‹è¯•åˆ†å±‚
- **å•å…ƒæµ‹è¯•**ï¼šæµ‹è¯•å•ä¸ªå‡½æ•°/æ–¹æ³•
- **é›†æˆæµ‹è¯•**ï¼šæµ‹è¯•ç»„ä»¶é—´çš„äº¤äº’
- **ç«¯åˆ°ç«¯æµ‹è¯•**ï¼šæµ‹è¯•å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹

### 3. æŒç»­é›†æˆ
```yaml
# .github/workflows/test.yml
name: Local Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'
    - name: Install dependencies
      run: pip install pytest pytest-mock
    - name: Run tests
      run: |
        python3 test/python/local_pipeline_test.py
        python3 test/python/data_validation_test.py
```

### 4. æµ‹è¯•æ•°æ®ç®¡ç†
```
test-data/
â”œâ”€â”€ small/          # å°æ•°æ®é›†ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
â”œâ”€â”€ medium/         # ä¸­ç­‰æ•°æ®é›†ï¼ˆåŠŸèƒ½æµ‹è¯•ï¼‰
â”œâ”€â”€ large/          # å¤§æ•°æ®é›†ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰
â””â”€â”€ edge-cases/     # è¾¹ç•Œæƒ…å†µæ•°æ®
```

## ğŸ“‹ æµ‹è¯•æ£€æŸ¥æ¸…å•

åœ¨éƒ¨ç½²åˆ°Dockerç¯å¢ƒå‰ï¼Œç¡®ä¿å®Œæˆä»¥ä¸‹æ£€æŸ¥ï¼š

- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡
- [ ] æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼ˆå°æ•°æ®é›†ï¼‰
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] ä»£ç è¦†ç›–ç‡è¾¾æ ‡
- [ ] æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

## ğŸ” å¸¸è§è°ƒè¯•åœºæ™¯

### åœºæ™¯1ï¼šMapperè¾“å‡ºæ ¼å¼é”™è¯¯
```python
# é”™è¯¯ç¤ºä¾‹
print(word, count)  # ç¼ºå°‘åˆ¶è¡¨ç¬¦åˆ†éš”

# æ­£ç¡®ç¤ºä¾‹
print(f"{word}\t{count}")
```

### åœºæ™¯2ï¼šReduceræ— æ³•æ­£ç¡®å¤„ç†åˆ†ç»„
```python
# é”™è¯¯ï¼šæ²¡æœ‰æŒ‰å•è¯åˆ†ç»„
for line in sys.stdin:
    word, count = line.strip().split('\t')
    total += int(count)
print(f"{word}\t{total}")  # wordå˜é‡å¯èƒ½æœªå®šä¹‰

# æ­£ç¡®ï¼šæŒ‰å•è¯åˆ†ç»„ç»Ÿè®¡
word_counts = defaultdict(int)
for line in sys.stdin:
    word, count = line.strip().split('\t')
    word_counts[word] += int(count)

for word, total in word_counts.items():
    print(f"{word}\t{total}")
```

### åœºæ™¯3ï¼šå†…å­˜ä½¿ç”¨è¿‡é«˜
```python
# ä¼˜åŒ–ï¼šä½¿ç”¨ç”Ÿæˆå™¨è€Œä¸æ˜¯ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®
def process_large_file(filename):
    with open(filename, 'r') as f:
        for line in f:
            yield line.strip()
```

é€šè¿‡è¿™å¥—å®Œæ•´çš„æœ¬åœ°Mockæµ‹è¯•ä½“ç³»ï¼Œä½ å¯ä»¥åœ¨éƒ¨ç½²åˆ°Dockerç¯å¢ƒä¹‹å‰å……åˆ†éªŒè¯ä»£ç çš„æ­£ç¡®æ€§ï¼Œå¤§å¤§å‡å°‘åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è°ƒè¯•çš„éš¾åº¦ã€‚