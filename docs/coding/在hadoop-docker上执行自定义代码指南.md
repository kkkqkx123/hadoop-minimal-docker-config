# åœ¨Hadoop-Dockerä¸Šæ‰§è¡Œè‡ªå®šä¹‰ä»£ç æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•åœ¨å½“å‰Hadoop-Dockerç¯å¢ƒä¸­æ‰§è¡Œè‡ªå®šä¹‰çš„MapReduceä»£ç ï¼ŒåŒ…æ‹¬Javaç¨‹åºã€Pythonè„šæœ¬ä»¥åŠå…¶ä»–ç±»å‹çš„ä½œä¸šã€‚

## ğŸ“‹ å‰ç½®æ¡ä»¶

ç¡®ä¿Hadoopé›†ç¾¤å·²æ­£å¸¸è¿è¡Œï¼š
```bash
# æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master jps"
```

åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¿›ç¨‹ï¼š
- NameNode
- ResourceManager
- SecondaryNameNode
- JobHistoryServer

## ğŸš€ æ‰§è¡Œè‡ªå®šä¹‰Java MapReduceç¨‹åº

### 1. å¼€å‘ç¯å¢ƒå‡†å¤‡

#### åœ¨æœ¬åœ°å¼€å‘
åœ¨Windowsä¸Šåˆ›å»ºJavaé¡¹ç›®ï¼Œæ·»åŠ Hadoopä¾èµ–ï¼š

```xml
<!-- pom.xml ä¾èµ–é…ç½® -->
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.6</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.6</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-mapreduce-client-core</artifactId>
        <version>3.3.6</version>
    </dependency>
</dependencies>
```

#### ç¤ºä¾‹WordCountç¨‹åº

```java
package com.example;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
    
    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) 
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

### 2. åœ¨å®¹å™¨ä¸­æ‰§è¡Œ

#### æ–¹æ³•1ï¼šå°†ä»£ç å¤åˆ¶åˆ°å®¹å™¨

```bash
# å°†JARæ–‡ä»¶å¤åˆ¶åˆ°masterå®¹å™¨
wsl -e bash -cl "docker cp /mnt/d/your-project/target/wordcount.jar hadoop-master:/tmp/"

# åœ¨å®¹å™¨ä¸­æ‰§è¡Œ
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hadoop jar /tmp/wordcount.jar com.example.WordCount /input /output"
```

#### æ–¹æ³•2ï¼šä½¿ç”¨æŒ‚è½½ç›®å½•

å°†ä»£ç æ”¾åœ¨æŒ‚è½½ç›®å½•ä¸­ï¼š

```bash
# åˆ›å»ºæµ‹è¯•æ•°æ®
echo "Hello World Hello Hadoop" > test.txt
echo "Hadoop MapReduce Example" >> test.txt

# ä¸Šä¼ åˆ°HDFS
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hdfs dfs -mkdir -p /input"
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hdfs dfs -put test.txt /input/"

# æ‰§è¡ŒWordCount
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output"

# æŸ¥çœ‹ç»“æœ
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hdfs dfs -cat /output/part-r-*"
```

## ğŸ æ‰§è¡ŒPython MapReduceç¨‹åº

### ä½¿ç”¨Hadoop Streaming

#### ç¤ºä¾‹Mapper (mapper.py)

```python
#!/usr/bin/env python3
import sys
import re

# è¯»å–æ ‡å‡†è¾“å…¥
for line in sys.stdin:
    # ç§»é™¤éå­—æ¯å­—ç¬¦å¹¶è½¬æ¢ä¸ºå°å†™
    line = re.sub(r'[^a-zA-Z\s]', '', line.lower())
    # åˆ†å‰²å•è¯
    words = line.split()
    
    for word in words:
        if word:  # ç¡®ä¿å•è¯ä¸ä¸ºç©º
            print(f"{word}\t1")
```

#### ç¤ºä¾‹Reducer (reducer.py)

```python
#!/usr/bin/env python3
import sys

current_word = None
current_count = 0

for line in sys.stdin:
    line = line.strip()
    word, count = line.split('\t', 1)
    
    try:
        count = int(count)
    except ValueError:
        continue
    
    if current_word == word:
        current_count += count
    else:
        if current_word:
            print(f"{current_word}\t{current_count}")
        current_word = word
        current_count = count

# è¾“å‡ºæœ€åä¸€ä¸ªå•è¯
if current_word == word:
    print(f"{current_word}\t{current_count}")
```

#### æ‰§è¡ŒPython MapReduce

```bash
# ä¸Šä¼ Pythonè„šæœ¬åˆ°HDFS
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hdfs dfs -mkdir -p /scripts"
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker cp mapper.py hadoop-master:/tmp/"
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker cp reducer.py hadoop-master:/tmp/"

# ç¡®ä¿è„šæœ¬æœ‰æ‰§è¡Œæƒé™
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master chmod +x /tmp/mapper.py /tmp/reducer.py"

# ä½¿ç”¨Hadoop Streamingæ‰§è¡Œ
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
  -files /tmp/mapper.py,/tmp/reducer.py \
  -mapper 'python3 /tmp/mapper.py' \
  -reducer 'python3 /tmp/reducer.py' \
  -input /input \
  -output /python-output"

# æŸ¥çœ‹ç»“æœ
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master hdfs dfs -cat /python-output/part-*"
```

## ğŸ“Š èµ„æºè°ƒä¼˜å»ºè®®

### å†…å­˜é…ç½®

å½“å‰é›†ç¾¤é…ç½®ï¼ˆé€‚åˆå­¦ä¹ ç¯å¢ƒï¼‰ï¼š
- Map Taskå†…å­˜: 512MB
- Reduce Taskå†…å­˜: 512MB
- Application Masterå†…å­˜: 768MB

### å¹¶å‘åº¦è®¾ç½®

```xml
<!-- åœ¨mapred-site.xmlä¸­ -->
<property>
  <name>mapreduce.job.maps</name>
  <value>2</value>  <!-- å°é›†ç¾¤ä½¿ç”¨è¾ƒå°‘çš„mapä»»åŠ¡ -->
</property>

<property>
  <name>mapreduce.job.reduces</name>
  <value>1</value>  <!-- å°é›†ç¾¤ä½¿ç”¨è¾ƒå°‘çš„reduceä»»åŠ¡ -->
</property>
```

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹ä½œä¸šæ—¥å¿—

```bash
# æŸ¥çœ‹ResourceManageræ—¥å¿—
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose logs master | grep -i resourcemanager"

# æŸ¥çœ‹å…·ä½“ä½œä¸šçš„æ—¥å¿—
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master yarn logs -applicationId application_xxx"

# æŸ¥çœ‹NodeManageræ—¥å¿—
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose logs worker1 | grep -i nodemanager"
```

### å¸¸è§é—®é¢˜æ’æŸ¥

1. **è¿æ¥æ‹’ç»é”™è¯¯**: æ£€æŸ¥ResourceManageræ˜¯å¦è¿è¡Œ
2. **å†…å­˜ä¸è¶³**: è°ƒæ•´mapreduceä»»åŠ¡å†…å­˜é…ç½®
3. **æƒé™é—®é¢˜**: ç¡®ä¿HDFSç›®å½•æƒé™æ­£ç¡®
4. **ç±»æ‰¾ä¸åˆ°**: æ£€æŸ¥JARåŒ…æ˜¯å¦æ­£ç¡®ä¸Šä¼ 

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### ä½¿ç”¨YARN Web UI

è®¿é—® ResourceManager Web UI:
```
http://localhost:8088
```

### ä½¿ç”¨JobHistory Web UI

è®¿é—® JobHistory Server:
```
http://localhost:19888
```

### å‘½ä»¤è¡Œç›‘æ§

```bash
# æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master yarn node -list"

# æŸ¥çœ‹è¿è¡Œä¸­çš„åº”ç”¨
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master yarn application -list"

# æŸ¥çœ‹åº”ç”¨çŠ¶æ€
wsl -e bash -cl "cd /home/docker-compose/hadoop && docker-compose exec master yarn application -status application_id"
```

## ğŸš€ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰åˆ†åŒºå™¨

```java
public class CustomPartitioner extends Partitioner<Text, IntWritable> {
    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        // è‡ªå®šä¹‰åˆ†åŒºé€»è¾‘
        return (key.hashCode() & Integer.MAX_VALUE) % numPartitions;
    }
}
```

### è‡ªå®šä¹‰æ¯”è¾ƒå™¨

```java
public class CustomComparator extends WritableComparator {
    protected CustomComparator() {
        super(Text.class, true);
    }
    
    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {
        Text t1 = (Text) w1;
        Text t2 = (Text) w2;
        // è‡ªå®šä¹‰æ¯”è¾ƒé€»è¾‘
        return t1.toString().compareTo(t2.toString());
    }
}
```

## ğŸ“š å‚è€ƒèµ„æº

- [Hadoopå®˜æ–¹æ–‡æ¡£](https://hadoop.apache.org/docs/)
- [MapReduceæ•™ç¨‹](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
- [Hadoop Streamingæ–‡æ¡£](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html)

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å°æ–‡ä»¶å¤„ç†**: ä½¿ç”¨CombineFileInputFormatå¤„ç†å¤§é‡å°æ–‡ä»¶
2. **å†…å­˜è°ƒä¼˜**: æ ¹æ®æ•°æ®é‡è°ƒæ•´map/reduceå†…å­˜é…ç½®
3. **é”™è¯¯å¤„ç†**: æ·»åŠ é€‚å½“çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
4. **æµ‹è¯•**: å…ˆåœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œå†å¤„ç†å¤§æ•°æ®
5. **ç›‘æ§**: ä½¿ç”¨Web UIå’Œæ—¥å¿—ç›‘æ§ä½œä¸šæ‰§è¡Œæƒ…å†µ